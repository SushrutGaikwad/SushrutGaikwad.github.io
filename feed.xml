<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://sushrutgaikwad.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sushrutgaikwad.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-14T07:58:27+00:00</updated><id>https://sushrutgaikwad.github.io/feed.xml</id><title type="html">Sushrut Gaikwad</title><subtitle>A Data Science graduate student exploring ML, AI, and statistical thinking. </subtitle><entry><title type="html">McCulloch-Pitts Neuron</title><link href="https://sushrutgaikwad.github.io/blog/2025/mcculloch-pitts-neuron/" rel="alternate" type="text/html" title="McCulloch-Pitts Neuron"/><published>2025-06-12T00:00:00+00:00</published><updated>2025-06-12T00:00:00+00:00</updated><id>https://sushrutgaikwad.github.io/blog/2025/mcculloch-pitts-neuron</id><content type="html" xml:base="https://sushrutgaikwad.github.io/blog/2025/mcculloch-pitts-neuron/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>McCulloch (a neuroscientist) and Pitts (a logician) proposed a highly simplified computational model of the biological neuron in 1943, known as the <strong>McCulloch-Pitts (MP) neuron</strong>, in their seminal paper <d-cite key="mcculloch1943logical"></d-cite> <a class="citation" href="#mcculloch1943logical">(McCulloch &amp; Pitts, 1943)</a>. The goal was to try and mimic the biological neuron and check if some simple functions can be implemented.</p> <h1 id="thresholding-logic">Thresholding Logic</h1> <p>Consider \(d\) binary input features represented as \(x_1\), \(x_2\), \(\dots\), \(x_d\), and a binary output \(y\). This is shown in <a href="#fig-general-mp-neuron">Figure 1</a>.</p> <figure id="fig-general-mp-neuron"> <div style="text-align: center;"> <img src="/assets/img/blog_posts/mp_neuron/general_mp_neuron.png" alt="A general McCulloch-Pitts (MP) neuron." style="width:30%"/> </div> <figcaption style="text-align: center;">Figure 1: A general McCulloch-Pitts (MP) neuron.</figcaption> </figure> <p>What this model does is something very simple. It first aggregates all the inputs, which is indicated by \(g\), and then applies a basic threshold function \(f\) on this aggregation. More precisely, the following is what happens. The inputs \(x_1\), \(x_2\), \(\dots\), \(x_d\) can be excitatory or inhibitory.</p> <ul> <li>If any of the inputs \(x_j\) (where \(j\in \{1, 2, \dots, d\}\)) is inhibitory: <ul> <li>Then the output is \(y=0\)</li> </ul> </li> <li>Else if none of the inputs \(x_j\) are inhibitory: <ul> <li> <p>The function \(g\) aggregates the inputs, i.e.,</p> \[\begin{equation}\label{eq:mp_neuron_aggregation} g\left(x_1, x_2, \dots, x_d\right) = g\left(\mathbf{x}\right) = \sum_{j=1}^{d} x_j, \end{equation}\] <p>and the output \(y\) is given by:</p> \[\begin{equation}\label{eq:mp_neuron_thresholding_logic} y = f (g (\mathbf{x})) = \begin{cases} 1, &amp; \text{if $g(\mathbf{x}) \geq \theta$},\\ 0, &amp; \text{if $g(\mathbf{x}) &lt; \theta$}. \end{cases} \end{equation}\] <p>This can be more simply written as:</p> \[\begin{equation}\label{eq:mp_neuron_thresholding_logic_simpler} y = f \left(x_1, \dots, x_d\right) = \begin{cases} 1, &amp; \text{if $x_1 + \cdots + x_d \geq \theta$},\\ 0, &amp; \text{if $x_1 + \cdots + x_d &lt; \theta$}. \end{cases} \end{equation}\] <p>Here, \(\theta\) is called the <strong>thresholding parameter</strong>. So, the output \(y\) is \(1\) if the aggregation (or the sum) of all the inputs is greater than or equal to the thresholding parameter \(\theta\), and \(0\) otherwise.</p> </li> </ul> </li> </ul> <p>Note that in Equation \eqref{eq:mp_neuron_thresholding_logic}, we are representing all the inputs \(x_1\), \(x_2\), \(\dots\), \(x_d\) collectively in the vector \(\mathbf{x}\), which is given by</p> \[\mathbf{x} = \begin{bmatrix} x_1\\ x_2\\ \vdots\\ x_d \end{bmatrix}\] <p>To understand the thresholding logic, consider the example of learning when you would like a movie given some of its features (or inputs) that are binary. You are given a data of movies. The output \(y\) of a movie is \(1\) if you like the it, and \(0\) if you donâ€™t. The binary features can be</p> \[\begin{align*} x_1 &amp;= \text{Is the director Nolan?}\\ x_2 &amp;= \text{Is the movie Sci-Fi?}\\ x_3 &amp;= \text{Is the IMDB rating $&gt;8$?}\\ \vdots \end{align*}\] <p>Think of these features as favourable checks. In other words, if more of these binary features have the value \(1\), it is more likely that you would like the movie. The thresholding parameter \(\theta\) here models your personality. If you are someone who would like any movie, then \(\theta\) would be lower for you. However, if you like the movie only when most of your favourable checks are satisfied, then \(\theta\) would be higher for you.</p> <h1 id="implementing-boolean-functions">Implementing Boolean Functions</h1> <p>What it means to implement a boolean function is the following. Consider the AND boolean function. The output of this function is \(1\) only if all the inputs are \(1\). Else, the output is \(0\). So, an MP neuron will be able to implement this boolean function if it exactly models this behavior.</p> <h2 id="and-function">AND Function</h2> <p>Recall that the equation for an MP neuron is given by Equation \eqref{eq:mp_neuron_thresholding_logic_simpler}. Consider that there are \(3\) boolean inputs \(x_1\), \(x_2\), and \(x_3\), and all of them are excitatory. So, the MP neuron modeling the AND function should have three inputs. Hence, substituting \(d=3\) in Equation \eqref{eq:mp_neuron_thresholding_logic_simpler}, we get</p> \[y = f \left(x_1, x_2, x_3\right) = \begin{cases} 1, &amp; \text{if $x_1 + x_2 + x_3 \geq \theta$},\\ 0, &amp; \text{if $x_1 + x_2 + x_3 &lt; \theta$}. \end{cases}\] <p>Now, the AND function outputs \(1\) only if <em>all the inputs</em> are \(1\). If all the outputs are \(1\), we will have their sum as</p> \[x_1 + x_2 + x_3 = 1 + 1 + 1 = 3\] <p>This clearly indicates that for an MP neuron modeling the AND function with three inputs, the value of the threshold \(\theta\) should be \(3\). Hence, substituting \(\theta = 3\), we get the equation of this MP neuron as</p> \[y = f \left(x_1, x_2, x_3\right) = \begin{cases} 1, &amp; \text{if $x_1 + x_2 + x_3 \geq 3$},\\ 0, &amp; \text{if $x_1 + x_2 + x_3 &lt; 3$}. \end{cases}\] <h2 id="or-function">OR Function</h2> <p>Again, recall that the equation for an MP neuron is given by Equation \eqref{eq:mp_neuron_thresholding_logic_simpler}. Consider that there are \(3\) boolean inputs \(x_1\), \(x_2\), and \(x_3\), and all of them are excitatory. So, the MP neuron modeling the OR function should have three inputs. Hence, substituting \(d=3\) in Equation \eqref{eq:mp_neuron_thresholding_logic_simpler}, we get</p> \[y = f \left(x_1, x_2, x_3\right) = \begin{cases} 1, &amp; \text{if $x_1 + x_2 + x_3 \geq \theta$},\\ 0, &amp; \text{if $x_1 + x_2 + x_3 &lt; \theta$}. \end{cases}\] <p>Now, the OR function outputs \(1\) only if <em>at least one</em> of the inputs is \(1\). If this is true, then the <em>least value</em> of their sum is</p> \[x_1 + x_2 + x_3 = 1\] <p>This clearly indicates that for an MP neuron modeling the OR function with three inputs, the value of the threshold \(\theta\) should be \(1\). Hence, substituting \(\theta = 1\), we get the equation of this MP neuron as</p> \[y = f \left(x_1, x_2, x_3\right) = \begin{cases} 1, &amp; \text{if $x_1 + x_2 + x_3 \geq 1$},\\ 0, &amp; \text{if $x_1 + x_2 + x_3 &lt; 1$}. \end{cases}\] <h2 id="not-function">NOT Function</h2> <p>For an MP neuron to model the NOT function, we will have to consider just a single input \(x_1\) that is inhibitory. We will have two cases:</p> <ul> <li>If \(x_1 = 1\), then \(y = 0\). This case is sorted as this is the exact behavior of the NOT function if the input is \(1\).</li> <li> <p>If \(x_1 = 0\), then using Equation \eqref{eq:mp_neuron_thresholding_logic_simpler} with \(d=1\), we get</p> \[y = f \left(x_1\right) = \begin{cases} 1, &amp; \text{if $x_1 \geq \theta$},\\ 0, &amp; \text{if $x_1 &lt; \theta$}. \end{cases}\] <p>As we have \(x_1=0\), and we want the output \(y\) to be the opposite, i.e., \(y=1\) if \(x_1 = 0\), we can model it by assigning \(\theta = 0\). Hence, substituting \(\theta = 0\), we get the equation of this MP neuron in this case as</p> \[y = f \left(x_1\right) = \begin{cases} 1, &amp; \text{if $x_1 \geq 0$},\\ 0, &amp; \text{if $x_1 &lt; 0$}. \end{cases}\] </li> </ul> <p>Can any boolean function be represented using an MP neuron? To answer this, we will first have to understand the geometric interpretation of an MP neuron.</p> <h1 id="geometric-interpretation-of-an-mp-neuron">Geometric Interpretation of an MP Neuron</h1> <p>Consider an MP neuron implementing the OR function with two inputs \(x_1\) and \(x_2\). The four possible inputs to the neuron are: \((0, 0)\), \((0, 1)\), \((1, 0)\), and \((1, 1)\). The output, using Equation \eqref{eq:mp_neuron_thresholding_logic_simpler} with \(d=2\) and \(\theta = 1\), is given by</p> \[y = \begin{cases} 1, &amp; \text{if $x_1 + x_2 \geq 1$},\\ 0, &amp; \text{if $x_1 + x_2 &lt; 1$}. \end{cases}\] <p>Clearly, we get a linear decision boundary given by the equation</p> \[x_1 + x_2 = 1.\] <p>Any input point lying on or in the positive half space of this boundary has an output of \(1\), and any input point lying in the negative half space of this boundary has an output of \(0\). <a href="#or-function-2-inputs-mp-neuron">Figure 2</a> shows this.</p> <figure id="or-function-2-inputs-mp-neuron"> <div style="text-align: center;"> <img src="/assets/img/blog_posts/mp_neuron/or_function_2_inputs_mp_neuron.png" alt="An MP neuron implementing an OR function with two inputs. The green points have an output of 1 and the red point has an output of 0." style="width:50%"/> </div> <figcaption style="text-align: center;">Figure 2: An MP neuron implementing an OR function with two inputs. The green points have an output of 1 and the red point has an output of 0.</figcaption> </figure> <p>So, a single MP neuron splits the input points into two halves using a linear boundary, or more generally a hyperplane, given by</p> \[x_1 + x_2 + \cdots + x_d = \theta,\] <p>where \(d\) is the number of inputs. All inputs that produce an output of \(0\) will be on one side of this hyperplane, and all the inputs that produce an output of \(1\) will be on the other side of this hyperplane. In other words, a single MP neuron can be used to represent boolean functions that are <em>linearly separable</em>. Linear separability for boolean function means that there exists a line (plane) such that all inputs with an output of \(1\) line on one side of this line (plane), and all inputs with an output of \(0\) lie on the other side of this line (plane).</p> <h1 id="questions-to-ponder">Questions to Ponder</h1> <p>Are there any non-separable boolean functions? If so, can they be represented using an MP neuron? We will address these questions eventually. More precisely, we would like to address the following questions:</p> <ul> <li>What about non-boolean (say, real) inputs?</li> <li>Do we always need to hand code the threshold?</li> <li>Are all inputs equally important?</li> <li>What about functions that are not linearly separable?</li> </ul> <h1 id="acknowledgment">Acknowledgment</h1> <p>I have referred to the YouTube playlists <a class="citation" href="#iitm-deep-learning-playlist">(IIT Madras - B.S. Degree Programme, 2023)</a> and <a class="citation" href="#nptel-deep-learning-playlist">(NPTEL-NOC IITM, 2019)</a> to write this blog.</p>]]></content><author><name>Sushrut</name></author><category term="deep learning"/><category term="neural network"/><category term="mp-neuron"/><category term="thresholding"/><category term="boolean-functions"/><category term="blog"/><summary type="html"><![CDATA[Introduction]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sushrutgaikwad.github.io/assets/img/blog_posts/mp_neuron/thumbnail.png"/><media:content medium="image" url="https://sushrutgaikwad.github.io/assets/img/blog_posts/mp_neuron/thumbnail.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry></feed>